{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50248, 4)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "all_tweets = pd.read_csv('/home/muskan_2117/Downloads/SemEval2017_train_data.csv')\n",
    "all_tweets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.600000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus, tomorrow is a ver...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.630000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>@mariakaykay aga tayo tomorrow ah. :) Good nig...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.600000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tina Fey &amp;amp; Amy Poehler are hosting the Gol...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.630000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Lunch from my new Lil spot ...THE COTTON BOWL ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>SNC Halloween Pr. Pumped. Let's work it for Su...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id     label                                               text  \\\n",
       "0  2.600000e+17   neutral  Won the match #getin . Plus, tomorrow is a ver...   \n",
       "1  2.640000e+17   neutral  Some areas of New England could see the first ...   \n",
       "2  2.640000e+17  negative                                      Not Available   \n",
       "3  2.640000e+17   neutral                                      Not Available   \n",
       "4  2.640000e+17   neutral                                      Not Available   \n",
       "5  2.640000e+17  positive                                      Not Available   \n",
       "6  2.630000e+17  positive  @mariakaykay aga tayo tomorrow ah. :) Good nig...   \n",
       "7  2.600000e+17   neutral  Tina Fey &amp; Amy Poehler are hosting the Gol...   \n",
       "8  2.630000e+17  positive  Lunch from my new Lil spot ...THE COTTON BOWL ...   \n",
       "9  2.640000e+17  positive  SNC Halloween Pr. Pumped. Let's work it for Su...   \n",
       "\n",
       "  Unnamed: 3  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  \n",
       "5        NaN  \n",
       "6        NaN  \n",
       "7        NaN  \n",
       "8        NaN  \n",
       "9        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using text pre-processing tool 'ekphrasis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
      "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n"
     ]
    }
   ],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
    "        'time', 'url', 'date', 'number'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    fix_html=True,  # fix HTML tokens\n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    # corpus from which the word statistics are going to be used \n",
    "    # for spell correction\n",
    "    corrector=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "    \n",
    "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
    "    # the tokenizer, should take as input a string and return a list of tokens\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # list of dictionaries, for replacing tokens extracted from the text,\n",
    "    # with other expressions. You can pass more than one dictionaries.\n",
    "    dicts=[emoticons]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "NaNCount = 0\n",
    "\n",
    "for i in range(len(all_tweets)):\n",
    "    if all_tweets.text[i] == 'Not Available':\n",
    "        tokens.append(['NaN'])\n",
    "        NaNCount = NaNCount + 1\n",
    "    else:\n",
    "        tmp = text_processor.pre_process_doc(all_tweets.text[i])\n",
    "        tokens.append(tmp)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets['tokens'] = tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.600000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Won the match #getin . Plus, tomorrow is a ver...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[won, the, match, &lt;hashtag&gt;, get, in, &lt;/hashta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Some areas of New England could see the first ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[some, areas, of, new, england, could, see, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>negative</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NaN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NaN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NaN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[NaN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.630000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>@mariakaykay aga tayo tomorrow ah. :) Good nig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;user&gt;, aga, tayo, tomorrow, ah, ., &lt;happy&gt;, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.600000e+17</td>\n",
       "      <td>neutral</td>\n",
       "      <td>Tina Fey &amp;amp; Amy Poehler are hosting the Gol...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[tina, fey, &amp;, amy, poehler, are, hosting, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.630000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>Lunch from my new Lil spot ...THE COTTON BOWL ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[lunch, from, my, new, lil, spot, ., &lt;repeated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.640000e+17</td>\n",
       "      <td>positive</td>\n",
       "      <td>SNC Halloween Pr. Pumped. Let's work it for Su...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[&lt;allcaps&gt;, snc, &lt;/allcaps&gt;, halloween, pr, .,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id     label                                               text  \\\n",
       "0  2.600000e+17   neutral  Won the match #getin . Plus, tomorrow is a ver...   \n",
       "1  2.640000e+17   neutral  Some areas of New England could see the first ...   \n",
       "2  2.640000e+17  negative                                      Not Available   \n",
       "3  2.640000e+17   neutral                                      Not Available   \n",
       "4  2.640000e+17   neutral                                      Not Available   \n",
       "5  2.640000e+17  positive                                      Not Available   \n",
       "6  2.630000e+17  positive  @mariakaykay aga tayo tomorrow ah. :) Good nig...   \n",
       "7  2.600000e+17   neutral  Tina Fey &amp; Amy Poehler are hosting the Gol...   \n",
       "8  2.630000e+17  positive  Lunch from my new Lil spot ...THE COTTON BOWL ...   \n",
       "9  2.640000e+17  positive  SNC Halloween Pr. Pumped. Let's work it for Su...   \n",
       "\n",
       "  Unnamed: 3                                             tokens  \n",
       "0        NaN  [won, the, match, <hashtag>, get, in, </hashta...  \n",
       "1        NaN  [some, areas, of, new, england, could, see, th...  \n",
       "2        NaN                                              [NaN]  \n",
       "3        NaN                                              [NaN]  \n",
       "4        NaN                                              [NaN]  \n",
       "5        NaN                                              [NaN]  \n",
       "6        NaN  [<user>, aga, tayo, tomorrow, ah, ., <happy>, ...  \n",
       "7        NaN  [tina, fey, &, amy, poehler, are, hosting, the...  \n",
       "8        NaN  [lunch, from, my, new, lil, spot, ., <repeated...  \n",
       "9        NaN  [<allcaps>, snc, </allcaps>, halloween, pr, .,...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of 'Not Available' tweets from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([np.arange(len(all_tweets) - NaNCount)]*2).T\n",
    "tweets = pd.DataFrame(data, columns = ['label','tokens']) \n",
    "\n",
    "j = 0\n",
    "\n",
    "for i in range(len(all_tweets)):\n",
    "    if all_tweets.tokens[i] == ['NaN']:\n",
    "        continue\n",
    "    else:\n",
    "        tweets['label'][j] = all_tweets['label'][i]\n",
    "        tweets['tokens'][j] = all_tweets['tokens'][i]\n",
    "        j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total tweets 50248\n",
      "NaNCount 8313\n",
      "available tweets 41935\n"
     ]
    }
   ],
   "source": [
    "print('total tweets', len(all_tweets))\n",
    "print('NaNCount', NaNCount)\n",
    "print('available tweets', len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[won, the, match, &lt;hashtag&gt;, get, in, &lt;/hashta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[some, areas, of, new, england, could, see, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>[&lt;user&gt;, aga, tayo, tomorrow, ah, ., &lt;happy&gt;, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[tina, fey, &amp;, amy, poehler, are, hosting, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>[lunch, from, my, new, lil, spot, ., &lt;repeated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>[&lt;allcaps&gt;, snc, &lt;/allcaps&gt;, halloween, pr, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negative</td>\n",
       "      <td>[&lt;user&gt;, i, am, sorry, ,, i, heart, paris, is,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[manchester, united, will, try, to, return, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[going, to, a, bulls, game, with, aaliyah, &amp;, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[any, toon, fans, with, a, spare, ticket, for,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                             tokens\n",
       "0   neutral  [won, the, match, <hashtag>, get, in, </hashta...\n",
       "1   neutral  [some, areas, of, new, england, could, see, th...\n",
       "2  positive  [<user>, aga, tayo, tomorrow, ah, ., <happy>, ...\n",
       "3   neutral  [tina, fey, &, amy, poehler, are, hosting, the...\n",
       "4  positive  [lunch, from, my, new, lil, spot, ., <repeated...\n",
       "5  positive  [<allcaps>, snc, </allcaps>, halloween, pr, .,...\n",
       "6  negative  [<user>, i, am, sorry, ,, i, heart, paris, is,...\n",
       "7   neutral  [manchester, united, will, try, to, return, to...\n",
       "8   neutral  [going, to, a, bulls, game, with, aaliyah, &, ...\n",
       "9   neutral  [any, toon, fans, with, a, spare, ticket, for,..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41935, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pre-trained GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/home/muskan_2117/DAIICT/Sem2/IT550/project_datasets/datastories-semeval2017-task4-master/embeddings/glove.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = gensim.models.KeyedVectors.load_word2vec_format('/home/muskan_2117/DAIICT/Sem2/IT550/project_datasets/datastories-semeval2017-task4-master/embeddings/glove.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_embeddings = []\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    token = tweets['tokens'][i]\n",
    "    embeddings = []\n",
    "    for j in range(len(token)):\n",
    "        try:\n",
    "            embeddings.append(model[token[j]])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    all_embeddings.append(embeddings)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1\n",
    "        except KeyError:   # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "            \n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41935, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tweets), 300))\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    wordvec_arrays[i,:] = word_vector(tweets['tokens'][i], 300)\n",
    "    \n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41935"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['glove_embeddings'] = all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>glove_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[won, the, match, &lt;hashtag&gt;, get, in, &lt;/hashta...</td>\n",
       "      <td>[[-0.102421, -0.095505, 0.245394, -0.279824, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[some, areas, of, new, england, could, see, th...</td>\n",
       "      <td>[[-0.064235, 0.06055, 0.176825, -0.460392, -1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>[&lt;user&gt;, aga, tayo, tomorrow, ah, ., &lt;happy&gt;, ...</td>\n",
       "      <td>[[-0.588336, 0.265917, 0.95253, 0.381975, 0.33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[tina, fey, &amp;, amy, poehler, are, hosting, the...</td>\n",
       "      <td>[[0.135389, -0.255092, -0.065636, -0.166235, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>[lunch, from, my, new, lil, spot, ., &lt;repeated...</td>\n",
       "      <td>[[0.199795, -0.165797, 0.788851, -0.299232, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>[&lt;allcaps&gt;, snc, &lt;/allcaps&gt;, halloween, pr, .,...</td>\n",
       "      <td>[[0.876378, -0.987462, -0.733877, -2.11284, -1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negative</td>\n",
       "      <td>[&lt;user&gt;, i, am, sorry, ,, i, heart, paris, is,...</td>\n",
       "      <td>[[-0.588336, 0.265917, 0.95253, 0.381975, 0.33...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[manchester, united, will, try, to, return, to...</td>\n",
       "      <td>[[0.262762, -0.669081, 0.019064, 0.117824, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[going, to, a, bulls, game, with, aaliyah, &amp;, ...</td>\n",
       "      <td>[[0.169069, 0.392756, 0.091623, -0.193693, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[any, toon, fans, with, a, spare, ticket, for,...</td>\n",
       "      <td>[[0.461138, 0.068601, -0.367184, -0.320787, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                             tokens  \\\n",
       "0   neutral  [won, the, match, <hashtag>, get, in, </hashta...   \n",
       "1   neutral  [some, areas, of, new, england, could, see, th...   \n",
       "2  positive  [<user>, aga, tayo, tomorrow, ah, ., <happy>, ...   \n",
       "3   neutral  [tina, fey, &, amy, poehler, are, hosting, the...   \n",
       "4  positive  [lunch, from, my, new, lil, spot, ., <repeated...   \n",
       "5  positive  [<allcaps>, snc, </allcaps>, halloween, pr, .,...   \n",
       "6  negative  [<user>, i, am, sorry, ,, i, heart, paris, is,...   \n",
       "7   neutral  [manchester, united, will, try, to, return, to...   \n",
       "8   neutral  [going, to, a, bulls, game, with, aaliyah, &, ...   \n",
       "9   neutral  [any, toon, fans, with, a, spare, ticket, for,...   \n",
       "\n",
       "                                    glove_embeddings  \n",
       "0  [[-0.102421, -0.095505, 0.245394, -0.279824, -...  \n",
       "1  [[-0.064235, 0.06055, 0.176825, -0.460392, -1....  \n",
       "2  [[-0.588336, 0.265917, 0.95253, 0.381975, 0.33...  \n",
       "3  [[0.135389, -0.255092, -0.065636, -0.166235, 0...  \n",
       "4  [[0.199795, -0.165797, 0.788851, -0.299232, 0....  \n",
       "5  [[0.876378, -0.987462, -0.733877, -2.11284, -1...  \n",
       "6  [[-0.588336, 0.265917, 0.95253, 0.381975, 0.33...  \n",
       "7  [[0.262762, -0.669081, 0.019064, 0.117824, 0.7...  \n",
       "8  [[0.169069, 0.392756, 0.091623, -0.193693, 0.1...  \n",
       "9  [[0.461138, 0.068601, -0.367184, -0.320787, -0...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X = tweets.glove_embeddings\n",
    "X = tweets['glove_embeddings']\n",
    "#X = X.reshape(-1, 1)\n",
    "y = tweets.label\n",
    "#y = np.array(tweets['label'])\n",
    "#y = y.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(wordvec_df, y, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37322, 300)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4613, 300)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "labels = ['positive', 'neutral', 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_embeddings = []\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    token = tweets['tokens'][i]\n",
    "    embeddings = []\n",
    "    for j in range(len(token)):\n",
    "        try:\n",
    "            embd = model[token[j]]\n",
    "            minimum = np.amin(embd)\n",
    "            maximum = np.amax(embd)\n",
    "            tmp = np.array(embd)\n",
    "            for j in range(len(tmp)):\n",
    "                tmp[j] = (tmp[j] - minimum)/(maximum - minimum)    \n",
    "            embeddings.append(tmp)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    nb_embeddings.append(embeddings)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['nb_embeddings'] = nb_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tokens</th>\n",
       "      <th>glove_embeddings</th>\n",
       "      <th>nb_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[won, the, match, &lt;hashtag&gt;, get, in, &lt;/hashta...</td>\n",
       "      <td>[[-0.102421, -0.095505, 0.245394, -0.279824, -...</td>\n",
       "      <td>[[0.5127671, 0.5152366, 0.6369606, 0.4494222, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[some, areas, of, new, england, could, see, th...</td>\n",
       "      <td>[[-0.064235, 0.06055, 0.176825, -0.460392, -1....</td>\n",
       "      <td>[[0.4912498, 0.5357633, 0.57724124, 0.34993187...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>[&lt;user&gt;, aga, tayo, tomorrow, ah, ., &lt;happy&gt;, ...</td>\n",
       "      <td>[[-0.588336, 0.265917, 0.95253, 0.381975, 0.33...</td>\n",
       "      <td>[[0.485613, 0.58785176, 0.670027, 0.60174185, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[tina, fey, &amp;, amy, poehler, are, hosting, the...</td>\n",
       "      <td>[[0.135389, -0.255092, -0.065636, -0.166235, 0...</td>\n",
       "      <td>[[0.60720074, 0.3801686, 0.49032146, 0.4318314...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>[lunch, from, my, new, lil, spot, ., &lt;repeated...</td>\n",
       "      <td>[[0.199795, -0.165797, 0.788851, -0.299232, 0....</td>\n",
       "      <td>[[0.55579877, 0.4088209, 0.79261523, 0.3551764...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>[&lt;allcaps&gt;, snc, &lt;/allcaps&gt;, halloween, pr, .,...</td>\n",
       "      <td>[[0.876378, -0.987462, -0.733877, -2.11284, -1...</td>\n",
       "      <td>[[0.54221195, 0.26383027, 0.30170554, 0.095744...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negative</td>\n",
       "      <td>[&lt;user&gt;, i, am, sorry, ,, i, heart, paris, is,...</td>\n",
       "      <td>[[-0.588336, 0.265917, 0.95253, 0.381975, 0.33...</td>\n",
       "      <td>[[0.485613, 0.58785176, 0.670027, 0.60174185, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[manchester, united, will, try, to, return, to...</td>\n",
       "      <td>[[0.262762, -0.669081, 0.019064, 0.117824, 0.7...</td>\n",
       "      <td>[[0.5813098, 0.22103326, 0.48708934, 0.5252727...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[going, to, a, bulls, game, with, aaliyah, &amp;, ...</td>\n",
       "      <td>[[0.169069, 0.392756, 0.091623, -0.193693, 0.1...</td>\n",
       "      <td>[[0.5443364, 0.6200081, 0.5181369, 0.42161644,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[any, toon, fans, with, a, spare, ticket, for,...</td>\n",
       "      <td>[[0.461138, 0.068601, -0.367184, -0.320787, -0...</td>\n",
       "      <td>[[0.7392031, 0.59675884, 0.43862063, 0.4554572...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                             tokens  \\\n",
       "0   neutral  [won, the, match, <hashtag>, get, in, </hashta...   \n",
       "1   neutral  [some, areas, of, new, england, could, see, th...   \n",
       "2  positive  [<user>, aga, tayo, tomorrow, ah, ., <happy>, ...   \n",
       "3   neutral  [tina, fey, &, amy, poehler, are, hosting, the...   \n",
       "4  positive  [lunch, from, my, new, lil, spot, ., <repeated...   \n",
       "5  positive  [<allcaps>, snc, </allcaps>, halloween, pr, .,...   \n",
       "6  negative  [<user>, i, am, sorry, ,, i, heart, paris, is,...   \n",
       "7   neutral  [manchester, united, will, try, to, return, to...   \n",
       "8   neutral  [going, to, a, bulls, game, with, aaliyah, &, ...   \n",
       "9   neutral  [any, toon, fans, with, a, spare, ticket, for,...   \n",
       "\n",
       "                                    glove_embeddings  \\\n",
       "0  [[-0.102421, -0.095505, 0.245394, -0.279824, -...   \n",
       "1  [[-0.064235, 0.06055, 0.176825, -0.460392, -1....   \n",
       "2  [[-0.588336, 0.265917, 0.95253, 0.381975, 0.33...   \n",
       "3  [[0.135389, -0.255092, -0.065636, -0.166235, 0...   \n",
       "4  [[0.199795, -0.165797, 0.788851, -0.299232, 0....   \n",
       "5  [[0.876378, -0.987462, -0.733877, -2.11284, -1...   \n",
       "6  [[-0.588336, 0.265917, 0.95253, 0.381975, 0.33...   \n",
       "7  [[0.262762, -0.669081, 0.019064, 0.117824, 0.7...   \n",
       "8  [[0.169069, 0.392756, 0.091623, -0.193693, 0.1...   \n",
       "9  [[0.461138, 0.068601, -0.367184, -0.320787, -0...   \n",
       "\n",
       "                                       nb_embeddings  \n",
       "0  [[0.5127671, 0.5152366, 0.6369606, 0.4494222, ...  \n",
       "1  [[0.4912498, 0.5357633, 0.57724124, 0.34993187...  \n",
       "2  [[0.485613, 0.58785176, 0.670027, 0.60174185, ...  \n",
       "3  [[0.60720074, 0.3801686, 0.49032146, 0.4318314...  \n",
       "4  [[0.55579877, 0.4088209, 0.79261523, 0.3551764...  \n",
       "5  [[0.54221195, 0.26383027, 0.30170554, 0.095744...  \n",
       "6  [[0.485613, 0.58785176, 0.670027, 0.60174185, ...  \n",
       "7  [[0.5813098, 0.22103326, 0.48708934, 0.5252727...  \n",
       "8  [[0.5443364, 0.6200081, 0.5181369, 0.42161644,...  \n",
       "9  [[0.7392031, 0.59675884, 0.43862063, 0.4554572...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    global_minimum = 0\n",
    "    for word in tokens: \n",
    "        try:\n",
    "            embd = model_w2v[word]\n",
    "            #embd = model[token[j]]\n",
    "            minimum = np.amin(embd)\n",
    "            if minimum < global_minimum:\n",
    "                global_minimum = minimum\n",
    "        except KeyError:   # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "      \n",
    "    for word in tokens: \n",
    "        try:\n",
    "            #maximum = np.amax(embd)\n",
    "            tmp = np.array(embd)\n",
    "            for j in range(len(tmp)):\n",
    "                tmp[j] = tmp[j] + abs(global_minimum)\n",
    "            vec += tmp.reshape((1, size))\n",
    "            count += 1\n",
    "        except KeyError:   # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "            \n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41935, 300)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_wordvec_arrays = np.zeros((len(tweets), 300))\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    nb_wordvec_arrays[i,:] = nb_word_vector(tweets['tokens'][i], 300)\n",
    "    \n",
    "nb_wordvec_df = pd.DataFrame(nb_wordvec_arrays)\n",
    "nb_wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_nb = tweets.label\n",
    "\n",
    "X_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(nb_wordvec_df, y_nb, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37322, 300)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4613, 300)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_nb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.24      0.10      0.14       697\n",
      "     neutral       0.48      0.93      0.63      2097\n",
      "    negative       0.75      0.08      0.14      1819\n",
      "\n",
      "   micro avg       0.47      0.47      0.47      4613\n",
      "   macro avg       0.49      0.37      0.30      4613\n",
      "weighted avg       0.55      0.47      0.36      4613\n",
      "\n",
      "[[ 142 1549  128]\n",
      " [  36 1960  101]\n",
      " [  11  615   71]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "clf = ComplementNB()\n",
    "\n",
    "clf.fit(X_train_nb, y_train_nb)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.00      0.00      0.00       697\n",
      "     neutral       0.46      1.00      0.63      2097\n",
      "    negative       0.77      0.01      0.02      1819\n",
      "\n",
      "   micro avg       0.46      0.46      0.46      4613\n",
      "   macro avg       0.41      0.34      0.21      4613\n",
      "weighted avg       0.51      0.46      0.29      4613\n",
      "\n",
      "[[  17 1802    0]\n",
      " [   5 2092    0]\n",
      " [   0  697    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train_nb, y_train_nb)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solver = sag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='sag',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver='sag')\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.62      0.38      0.47       697\n",
      "     neutral       0.65      0.74      0.69      2097\n",
      "    negative       0.70      0.69      0.70      1819\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      4613\n",
      "   macro avg       0.66      0.61      0.62      4613\n",
      "weighted avg       0.67      0.67      0.66      4613\n",
      "\n",
      "[[1258  521   40]\n",
      " [ 415 1562  120]\n",
      " [ 112  321  264]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solver = newton-cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.63      0.38      0.47       697\n",
      "     neutral       0.65      0.75      0.69      2097\n",
      "    negative       0.70      0.69      0.70      1819\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      4613\n",
      "   macro avg       0.66      0.61      0.62      4613\n",
      "weighted avg       0.67      0.67      0.66      4613\n",
      "\n",
      "[[1260  519   40]\n",
      " [ 416 1563  118]\n",
      " [ 112  321  264]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solver = saga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver='saga')\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.63      0.38      0.47       697\n",
      "     neutral       0.65      0.74      0.69      2097\n",
      "    negative       0.70      0.69      0.70      1819\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      4613\n",
      "   macro avg       0.66      0.60      0.62      4613\n",
      "weighted avg       0.67      0.67      0.66      4613\n",
      "\n",
      "[[1255  525   39]\n",
      " [ 419 1559  119]\n",
      " [ 111  322  264]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solver = lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.62      0.38      0.47       697\n",
      "     neutral       0.64      0.73      0.69      2097\n",
      "    negative       0.70      0.69      0.69      1819\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      4613\n",
      "   macro avg       0.65      0.60      0.62      4613\n",
      "weighted avg       0.66      0.66      0.66      4613\n",
      "\n",
      "[[1255  526   38]\n",
      " [ 437 1537  123]\n",
      " [ 108  326  263]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=1)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel='linear', verbose = 1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.61      0.38      0.47       697\n",
      "     neutral       0.63      0.76      0.69      2097\n",
      "    negative       0.73      0.66      0.69      1819\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      4613\n",
      "   macro avg       0.66      0.60      0.62      4613\n",
      "weighted avg       0.67      0.67      0.66      4613\n",
      "\n",
      "[[1206  569   44]\n",
      " [ 373 1600  124]\n",
      " [  79  354  264]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Poly Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='poly', verbose = 1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.00      0.00      0.00       697\n",
      "     neutral       0.49      0.95      0.65      2097\n",
      "    negative       0.72      0.22      0.34      1819\n",
      "\n",
      "   micro avg       0.52      0.52      0.52      4613\n",
      "   macro avg       0.40      0.39      0.33      4613\n",
      "weighted avg       0.51      0.52      0.43      4613\n",
      "\n",
      "[[ 407 1412    0]\n",
      " [ 108 1989    0]\n",
      " [  51  646    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf', verbose = 1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.62      0.01      0.03       697\n",
      "     neutral       0.58      0.83      0.68      2097\n",
      "    negative       0.71      0.62      0.66      1819\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      4613\n",
      "   macro avg       0.64      0.49      0.46      4613\n",
      "weighted avg       0.64      0.62      0.58      4613\n",
      "\n",
      "[[1127  690    2]\n",
      " [ 352 1741    4]\n",
      " [ 106  581   10]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='sigmoid', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='sigmoid', verbose = 1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.50      0.00      0.01       697\n",
      "     neutral       0.57      0.83      0.68      2097\n",
      "    negative       0.70      0.59      0.64      1819\n",
      "\n",
      "   micro avg       0.61      0.61      0.61      4613\n",
      "   macro avg       0.59      0.47      0.44      4613\n",
      "weighted avg       0.61      0.61      0.56      4613\n",
      "\n",
      "[[1069  749    1]\n",
      " [ 347 1749    1]\n",
      " [ 116  579    2]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss = hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.65      0.34      0.45       697\n",
      "     neutral       0.65      0.74      0.69      2097\n",
      "    negative       0.69      0.70      0.69      1819\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      4613\n",
      "   macro avg       0.66      0.59      0.61      4613\n",
      "weighted avg       0.67      0.67      0.66      4613\n",
      "\n",
      "[[1281  498   40]\n",
      " [ 458 1552   87]\n",
      " [ 130  330  237]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss = modified_huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='modified_huber',\n",
       "       max_iter=1000, n_iter=None, n_iter_no_change=5, n_jobs=None,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       tol=None, validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss=\"modified_huber\", penalty=\"l2\", max_iter=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.64      0.34      0.45       697\n",
      "     neutral       0.63      0.78      0.70      2097\n",
      "    negative       0.73      0.65      0.69      1819\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      4613\n",
      "   macro avg       0.66      0.59      0.61      4613\n",
      "weighted avg       0.67      0.66      0.66      4613\n",
      "\n",
      "[[1187  592   40]\n",
      " [ 359 1641   97]\n",
      " [  87  371  239]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loss = log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "       l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
       "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "       validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=1000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.63      0.35      0.45       697\n",
      "     neutral       0.64      0.75      0.69      2097\n",
      "    negative       0.70      0.68      0.69      1819\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      4613\n",
      "   macro avg       0.66      0.59      0.61      4613\n",
      "weighted avg       0.66      0.66      0.66      4613\n",
      "\n",
      "[[1240  544   35]\n",
      " [ 416 1573  108]\n",
      " [ 107  344  246]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adbc = AdaBoostClassifier()\n",
    "\n",
    "adbc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.44      0.31      0.37       697\n",
      "     neutral       0.58      0.64      0.61      2097\n",
      "    negative       0.61      0.60      0.60      1819\n",
      "\n",
      "   micro avg       0.58      0.58      0.58      4613\n",
      "   macro avg       0.54      0.52      0.53      4613\n",
      "weighted avg       0.57      0.58      0.57      4613\n",
      "\n",
      "[[1092  643   84]\n",
      " [ 562 1347  188]\n",
      " [ 146  334  217]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = adbc.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk_tokens = []\n",
    "NaNCount = 0\n",
    "\n",
    "for i in range(len(all_tweets)):\n",
    "    if all_tweets.text[i] == 'Not Available':\n",
    "        nltk_tokens.append(['NaN'])\n",
    "        NaNCount = NaNCount + 1\n",
    "    else:\n",
    "        tmp = word_tokenize(str(all_tweets.text[i]))\n",
    "        nltk_tokens.append(tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets['nltk_tokens'] = nltk_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  del sys.path[0]\n",
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:189: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([np.arange(len(all_tweets) - NaNCount)]*2).T\n",
    "nltk_tweets = pd.DataFrame(data, columns = ['label','nltk_tokens']) \n",
    "\n",
    "j = 0\n",
    "\n",
    "for i in range(len(all_tweets)):\n",
    "    if all_tweets.nltk_tokens[i] == ['NaN']:\n",
    "        continue\n",
    "    else:\n",
    "        nltk_tweets['label'][j] = all_tweets['label'][i]\n",
    "        nltk_tweets['nltk_tokens'][j] = all_tweets['nltk_tokens'][i]\n",
    "        j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>nltk_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[Won, the, match, #, getin, ., Plus, ,, tomorr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[Some, areas, of, New, England, could, see, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>[@, mariakaykay, aga, tayo, tomorrow, ah, ., :...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[Tina, Fey, &amp;, amp, ;, Amy, Poehler, are, host...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>[Lunch, from, my, new, Lil, spot, ..., THE, CO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>positive</td>\n",
       "      <td>[SNC, Halloween, Pr, ., Pumped, ., Let, 's, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>negative</td>\n",
       "      <td>[@, jacquelinemegan, I, 'm, sorry, ,, I, Heart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[Manchester, United, will, try, to, return, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[Going, to, a, bulls, game, with, Aaliyah, &amp;, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neutral</td>\n",
       "      <td>[Any, Toon, Fans, with, a, spare, ticket, for,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                        nltk_tokens\n",
       "0   neutral  [Won, the, match, #, getin, ., Plus, ,, tomorr...\n",
       "1   neutral  [Some, areas, of, New, England, could, see, th...\n",
       "2  positive  [@, mariakaykay, aga, tayo, tomorrow, ah, ., :...\n",
       "3   neutral  [Tina, Fey, &, amp, ;, Amy, Poehler, are, host...\n",
       "4  positive  [Lunch, from, my, new, Lil, spot, ..., THE, CO...\n",
       "5  positive  [SNC, Halloween, Pr, ., Pumped, ., Let, 's, wo...\n",
       "6  negative  [@, jacquelinemegan, I, 'm, sorry, ,, I, Heart...\n",
       "7   neutral  [Manchester, United, will, try, to, return, to...\n",
       "8   neutral  [Going, to, a, bulls, game, with, Aaliyah, &, ...\n",
       "9   neutral  [Any, Toon, Fans, with, a, spare, ticket, for,..."
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_embeddings = []\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    token = nltk_tweets['nltk_tokens'][i]\n",
    "    embeddings = []\n",
    "    for j in range(len(token)):\n",
    "        try:\n",
    "            embeddings.append(model[token[j]])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    nltk_embeddings.append(embeddings)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41935, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_wordvec_arrays = np.zeros((len(nltk_tweets), 300))\n",
    "\n",
    "for i in range(len(nltk_tweets)):\n",
    "    nltk_wordvec_arrays[i,:] = word_vector(nltk_tweets['nltk_tokens'][i], 300)\n",
    "    \n",
    "nltk_wordvec_df = pd.DataFrame(nltk_wordvec_arrays)\n",
    "nltk_wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nltk, X_test_nltk, y_train_nltk, y_test_nltk = train_test_split(nltk_wordvec_df, y, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37322, 300)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nltk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4613, 300)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_nltk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.15      0.53      0.23       697\n",
      "     neutral       0.00      0.00      0.00      2097\n",
      "    negative       0.40      0.48      0.44      1819\n",
      "\n",
      "   micro avg       0.27      0.27      0.27      4613\n",
      "   macro avg       0.18      0.33      0.22      4613\n",
      "weighted avg       0.18      0.27      0.21      4613\n",
      "\n",
      "[[ 868    0  951]\n",
      " [ 968    0 1129]\n",
      " [ 331    0  366]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_nb = tweets.label\n",
    "\n",
    "X_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(nb_wordvec_df_nltk, y_nb, test_size=0.11, random_state=42)\n",
    "\n",
    "clf = ComplementNB()\n",
    "\n",
    "clf.fit(X_train_nb, y_train_nb)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.00      0.00      0.00       697\n",
      "     neutral       0.45      1.00      0.63      2097\n",
      "    negative       0.00      0.00      0.00      1819\n",
      "\n",
      "   micro avg       0.45      0.45      0.45      4613\n",
      "   macro avg       0.15      0.33      0.21      4613\n",
      "weighted avg       0.21      0.45      0.28      4613\n",
      "\n",
      "[[   0 1819    0]\n",
      " [   0 2097    0]\n",
      " [   0  697    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(X_train_nb, y_train_nb)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.59      0.28      0.37       697\n",
      "     neutral       0.60      0.78      0.67      2097\n",
      "    negative       0.70      0.60      0.65      1819\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      4613\n",
      "   macro avg       0.63      0.55      0.57      4613\n",
      "weighted avg       0.64      0.63      0.62      4613\n",
      "\n",
      "[[1093  685   41]\n",
      " [ 372 1630   95]\n",
      " [  86  419  192]]\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(X_train_nltk, y_train_nltk)\n",
    "y_pred_nltk = clf.predict(X_test_nltk)\n",
    "\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.00      0.00      0.00       697\n",
      "     neutral       0.48      0.96      0.64      2097\n",
      "    negative       0.76      0.17      0.27      1819\n",
      "\n",
      "   micro avg       0.50      0.50      0.50      4613\n",
      "   macro avg       0.41      0.38      0.30      4613\n",
      "weighted avg       0.52      0.50      0.40      4613\n",
      "\n",
      "[[ 304 1515    0]\n",
      " [  82 2015    0]\n",
      " [  14  683    0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(kernel='poly')\n",
    "clf.fit(X_train_nltk, y_train_nltk)\n",
    "y_pred_nltk = clf.predict(X_test_nltk)\n",
    "\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.53      0.01      0.03       697\n",
      "     neutral       0.56      0.84      0.67      2097\n",
      "    negative       0.70      0.56      0.62      1819\n",
      "\n",
      "   micro avg       0.61      0.61      0.61      4613\n",
      "   macro avg       0.60      0.47      0.44      4613\n",
      "weighted avg       0.61      0.61      0.56      4613\n",
      "\n",
      "[[1017  798    4]\n",
      " [ 324 1768    5]\n",
      " [ 107  580   10]]\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(X_train_nltk, y_train_nltk)\n",
    "y_pred_nltk = clf.predict(X_test_nltk)\n",
    "\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.33      0.01      0.01       697\n",
      "     neutral       0.55      0.86      0.67      2097\n",
      "    negative       0.70      0.51      0.59      1819\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      4613\n",
      "   macro avg       0.53      0.46      0.42      4613\n",
      "weighted avg       0.58      0.59      0.54      4613\n",
      "\n",
      "[[ 932  884    3]\n",
      " [ 294 1798    5]\n",
      " [ 103  590    4]]\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC(kernel='sigmoid')\n",
    "clf.fit(X_train_nltk, y_train_nltk)\n",
    "y_pred_nltk = clf.predict(X_test_nltk)\n",
    "\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.60      0.32      0.42       697\n",
      "     neutral       0.61      0.74      0.67      2097\n",
      "    negative       0.68      0.64      0.66      1819\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      4613\n",
      "   macro avg       0.63      0.57      0.58      4613\n",
      "weighted avg       0.64      0.64      0.63      4613\n",
      "\n",
      "[[1161  617   41]\n",
      " [ 443 1550  104]\n",
      " [ 115  360  222]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(solver='sag')\n",
    "\n",
    "classifier.fit(X_train_nltk, y_train_nltk)\n",
    "\n",
    "y_pred_nltk = classifier.predict(X_test_nltk)\n",
    "\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.60      0.32      0.42       697\n",
      "     neutral       0.61      0.74      0.67      2097\n",
      "    negative       0.68      0.64      0.66      1819\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      4613\n",
      "   macro avg       0.63      0.57      0.58      4613\n",
      "weighted avg       0.64      0.64      0.63      4613\n",
      "\n",
      "[[1160  618   41]\n",
      " [ 441 1551  105]\n",
      " [ 114  360  223]]\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver='newton-cg')\n",
    "\n",
    "classifier.fit(X_train_nltk, y_train_nltk)\n",
    "\n",
    "y_pred_nltk = classifier.predict(X_test_nltk)\n",
    "\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.61      0.32      0.42       697\n",
      "     neutral       0.61      0.74      0.67      2097\n",
      "    negative       0.68      0.64      0.66      1819\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      4613\n",
      "   macro avg       0.63      0.57      0.58      4613\n",
      "weighted avg       0.64      0.64      0.63      4613\n",
      "\n",
      "[[1163  616   40]\n",
      " [ 442 1551  104]\n",
      " [ 116  360  221]]\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver='saga')\n",
    "\n",
    "classifier.fit(X_train_nltk, y_train_nltk)\n",
    "\n",
    "y_pred_nltk = classifier.predict(X_test_nltk)\n",
    "\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.61      0.32      0.42       697\n",
      "     neutral       0.62      0.74      0.67      2097\n",
      "    negative       0.68      0.64      0.66      1819\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      4613\n",
      "   macro avg       0.63      0.57      0.58      4613\n",
      "weighted avg       0.64      0.64      0.63      4613\n",
      "\n",
      "[[1166  616   37]\n",
      " [ 439 1552  106]\n",
      " [ 118  355  224]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "classifier.fit(X_train_nltk, y_train_nltk)\n",
    "\n",
    "y_pred_nltk = classifier.predict(X_test_nltk)\n",
    "\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.74      0.12      0.21       697\n",
      "     neutral       0.61      0.75      0.67      2097\n",
      "    negative       0.65      0.67      0.66      1819\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      4613\n",
      "   macro avg       0.66      0.52      0.51      4613\n",
      "weighted avg       0.64      0.63      0.60      4613\n",
      "\n",
      "[[1225  586    8]\n",
      " [ 493 1582   22]\n",
      " [ 172  440   85]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000)\n",
    "clf.fit(X_train_nltk, y_train_nltk)\n",
    "y_pred_nltk = clf.predict(X_test_nltk)\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.61      0.28      0.38       697\n",
      "     neutral       0.63      0.70      0.66      2097\n",
      "    negative       0.64      0.70      0.67      1819\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      4613\n",
      "   macro avg       0.63      0.56      0.57      4613\n",
      "weighted avg       0.63      0.63      0.62      4613\n",
      "\n",
      "[[1272  513   34]\n",
      " [ 550 1458   89]\n",
      " [ 166  337  194]]\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"modified_huber\", penalty=\"l2\", max_iter=1000)\n",
    "clf.fit(X_train_nltk, y_train_nltk)\n",
    "y_pred_nltk = clf.predict(X_test_nltk)\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muskan_2117/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.61      0.28      0.39       697\n",
      "     neutral       0.61      0.75      0.67      2097\n",
      "    negative       0.68      0.64      0.66      1819\n",
      "\n",
      "   micro avg       0.64      0.64      0.64      4613\n",
      "   macro avg       0.63      0.56      0.57      4613\n",
      "weighted avg       0.64      0.64      0.62      4613\n",
      "\n",
      "[[1167  618   34]\n",
      " [ 439 1568   90]\n",
      " [ 121  378  198]]\n"
     ]
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=1000)\n",
    "clf.fit(X_train_nltk, y_train_nltk)\n",
    "y_pred_nltk = clf.predict(X_test_nltk)\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.42      0.23      0.30       697\n",
      "     neutral       0.57      0.68      0.62      2097\n",
      "    negative       0.60      0.57      0.59      1819\n",
      "\n",
      "   micro avg       0.57      0.57      0.57      4613\n",
      "   macro avg       0.53      0.49      0.50      4613\n",
      "weighted avg       0.56      0.57      0.56      4613\n",
      "\n",
      "[[1037  703   79]\n",
      " [ 524 1433  140]\n",
      " [ 163  375  159]]\n"
     ]
    }
   ],
   "source": [
    "adbc = AdaBoostClassifier()\n",
    "\n",
    "adbc.fit(X_train_nltk, y_train_nltk)\n",
    "\n",
    "y_pred_nltk = adbc.predict(X_test_nltk)\n",
    "\n",
    "print(classification_report(y_test_nltk, y_pred_nltk, target_names=labels))\n",
    "print(confusion_matrix(y_test_nltk, y_pred_nltk, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ekphrasis vs nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<user>',\n",
       " 'aga',\n",
       " 'tayo',\n",
       " 'tomorrow',\n",
       " 'ah',\n",
       " '.',\n",
       " '<happy>',\n",
       " 'good',\n",
       " 'night',\n",
       " ',',\n",
       " 'ces',\n",
       " '.',\n",
       " 'love',\n",
       " 'you',\n",
       " '!',\n",
       " '<devil>',\n",
       " '<']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['tokens'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'mariakaykay',\n",
       " 'aga',\n",
       " 'tayo',\n",
       " 'tomorrow',\n",
       " 'ah',\n",
       " '.',\n",
       " ':',\n",
       " ')',\n",
       " 'Good',\n",
       " 'night',\n",
       " ',',\n",
       " 'Ces',\n",
       " '.',\n",
       " 'Love',\n",
       " 'you',\n",
       " '!',\n",
       " '&',\n",
       " 'gt',\n",
       " ';',\n",
       " ':',\n",
       " 'D',\n",
       " '&',\n",
       " 'lt',\n",
       " ';']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_tweets['nltk_tokens'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.748700e-02,  1.497360e-01,  5.326670e-01, -1.560000e-01,\n",
       "       -5.435400e-02, -7.420520e-01,  3.741750e-01, -3.096200e-02,\n",
       "       -4.451240e-01,  2.979030e-01,  1.190787e+00, -2.323090e-01,\n",
       "        2.707480e-01,  3.577600e-02, -2.784010e-01, -2.966300e-02,\n",
       "        2.873500e-01,  5.789290e-01, -1.085148e+00,  4.069050e-01,\n",
       "       -7.866730e-01, -1.962260e-01, -2.119950e-01, -1.212554e+00,\n",
       "        3.784580e-01,  4.365000e-02, -3.278020e-01,  4.973920e-01,\n",
       "        6.721110e-01,  8.218180e-01,  6.863920e-01, -5.409120e-01,\n",
       "       -6.702300e-02,  6.368500e-02, -3.590410e-01,  1.859400e-01,\n",
       "       -2.181380e-01,  3.023610e-01, -6.591900e-02,  4.849880e-01,\n",
       "        8.560620e-01,  1.249526e+00, -1.777450e-01,  7.323290e-01,\n",
       "        4.834900e-01,  2.590170e-01, -1.078250e-01,  7.514200e-02,\n",
       "        7.747760e-01,  3.823860e-01, -9.880800e-02, -7.663040e-01,\n",
       "       -2.038410e-01,  1.536300e-01,  1.214650e-01, -1.225600e-01,\n",
       "        4.677220e-01,  4.571680e-01,  4.447220e-01, -4.920670e-01,\n",
       "        6.663100e-01,  8.236640e-01, -2.282570e-01,  3.267500e-01,\n",
       "        1.274072e+00,  3.678260e-01,  1.130905e+00, -9.050800e-02,\n",
       "       -8.519010e-01,  2.615550e-01, -7.106460e-01,  3.496160e-01,\n",
       "       -2.993030e-01,  3.491820e-01, -3.230000e-04, -1.776000e-02,\n",
       "       -1.045426e+00,  4.430570e-01,  3.743930e-01,  1.852050e-01,\n",
       "       -9.801210e-01, -7.316790e-01,  3.994640e-01, -1.949430e-01,\n",
       "       -1.044198e+00, -9.272200e-02, -2.048970e-01, -1.046000e-03,\n",
       "       -4.158370e-01,  8.964100e-02,  6.590090e-01, -5.360220e-01,\n",
       "        3.188270e-01, -4.446700e-01, -1.027886e+00,  2.184000e-03,\n",
       "       -1.512720e-01, -1.013530e-01, -3.691250e-01, -5.401660e-01,\n",
       "        2.265420e-01,  4.318630e-01,  7.433930e-01, -1.910700e-01,\n",
       "       -3.407510e-01,  7.807530e-01,  7.443250e-01, -2.742220e-01,\n",
       "        1.414000e-02,  2.638910e-01,  1.726680e-01,  4.757300e-02,\n",
       "       -3.062720e-01, -1.062279e+00, -8.230850e-01,  3.929840e-01,\n",
       "       -9.233340e-01, -9.784400e-01, -1.478473e+00, -1.467530e-01,\n",
       "        1.348100e-01,  7.174240e-01, -5.832130e-01, -1.685130e-01,\n",
       "        3.874920e-01, -9.509120e-01,  3.076840e-01, -3.383580e-01,\n",
       "       -6.334580e-01,  3.362000e-03,  3.653180e-01, -8.225570e-01,\n",
       "        4.600300e-02,  3.992570e-01,  2.679850e-01, -1.920120e-01,\n",
       "       -3.849250e-01, -4.598680e-01,  2.571750e-01,  1.277750e-01,\n",
       "       -3.544550e-01,  1.936800e-02, -6.171860e-01, -3.282730e-01,\n",
       "        7.657500e-01,  3.734120e-01,  4.816200e-02, -9.766240e-01,\n",
       "        3.301760e-01,  9.158600e-01, -6.073930e-01,  7.586370e-01,\n",
       "        2.957360e-01, -7.764380e-01, -1.232350e-01, -3.121460e-01,\n",
       "        3.864010e-01, -1.787920e-01, -8.488940e-01,  7.949320e-01,\n",
       "        2.182560e-01, -7.953030e-01,  6.291910e-01, -2.955660e-01,\n",
       "        6.287120e-01,  7.604490e-01,  7.424490e-01, -1.464870e-01,\n",
       "        7.977210e-01,  1.388560e-01, -2.653380e-01, -7.001300e-02,\n",
       "       -1.030199e+00, -1.032660e-01,  3.342970e-01, -2.732760e-01,\n",
       "       -1.059559e+00, -1.303110e-01,  4.517880e-01, -1.187700e-01,\n",
       "        3.454710e-01,  6.113300e-02, -9.598210e-01, -9.760350e-01,\n",
       "        6.980940e-01,  2.668270e-01,  1.272880e-01,  1.669980e-01,\n",
       "       -6.515480e-01,  3.221910e-01, -1.650020e-01,  6.229270e-01,\n",
       "       -6.832000e-01, -3.953510e-01, -3.901400e-01, -1.861200e-02,\n",
       "       -2.491170e-01,  7.814760e-01, -8.702800e-02, -9.707020e-01,\n",
       "        1.110050e-01, -3.095320e-01,  6.327590e-01,  4.925400e-02,\n",
       "       -5.938700e-02, -7.258400e-01, -4.572710e-01, -2.749760e-01,\n",
       "        9.445420e-01, -2.613140e-01, -2.928500e-01,  4.544480e-01,\n",
       "        6.149000e-02, -3.083420e-01, -5.835330e-01, -4.540110e-01,\n",
       "       -4.014890e-01, -1.321620e-01,  6.221070e-01, -5.103860e-01,\n",
       "        5.108680e-01,  2.833560e-01, -8.638600e-02, -5.383200e-01,\n",
       "       -2.555360e-01, -7.610670e-01, -1.951370e-01, -6.735100e-02,\n",
       "       -9.956200e-02, -1.130499e+00,  1.867920e-01, -6.301610e-01,\n",
       "        3.177070e-01, -2.937960e-01,  3.333600e-02, -2.828840e-01,\n",
       "       -1.226190e-01,  1.252310e-01,  5.120460e-01,  1.924400e-01,\n",
       "        5.893260e-01,  9.319100e-02, -2.158250e-01,  5.277590e-01,\n",
       "        2.450200e-01, -6.667340e-01, -3.902120e-01,  7.568800e-02,\n",
       "        1.451560e-01, -1.403440e-01,  7.497600e-01, -2.564900e-01,\n",
       "       -1.930590e-01,  3.028940e-01,  6.602300e-02, -4.642700e-02,\n",
       "        7.115100e-02,  6.261600e-02, -8.997300e-02,  1.800437e+00,\n",
       "        8.818010e-01, -9.528120e-01,  1.449620e-01,  6.069290e-01,\n",
       "        4.899520e-01,  2.011850e-01, -4.886000e-01,  2.094730e-01,\n",
       "        2.014260e-01, -4.821200e-01, -5.772400e-02,  2.702870e-01,\n",
       "        9.976260e-01, -1.989330e-01,  4.831270e-01, -6.715740e-01,\n",
       "        3.800850e-01, -5.259300e-02,  2.159520e-01,  7.764090e-01,\n",
       "        1.211370e+00,  1.016860e-01,  5.268590e-01, -2.593000e-02,\n",
       "       -2.643080e-01, -2.914300e-01,  1.367510e-01,  3.879000e-02,\n",
       "       -6.285440e-01,  9.375230e-01,  1.668990e-01, -6.653610e-01,\n",
       "        1.364520e-01,  1.841400e-01, -7.740460e-01, -4.213640e-01,\n",
       "       -1.100526e+00,  8.871900e-02, -8.684490e-01, -1.240486e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['<happy>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.116521,  0.15463 ,  0.047883,  0.063743,  0.112395, -0.065712,\n",
       "       -0.044426, -0.011419,  0.098808,  0.042539, -0.110289,  0.019689,\n",
       "       -0.009833,  0.028707, -0.081217, -0.081131, -0.234737, -0.071947,\n",
       "        0.030969,  0.135651,  0.056755, -0.148565, -0.055295, -0.180719,\n",
       "       -0.070928, -0.029374, -0.022661, -0.001052,  0.022075, -0.074194,\n",
       "       -0.091624,  0.099024,  0.241266, -0.003809, -0.04262 , -0.24047 ,\n",
       "        0.01284 , -0.184066, -0.04188 , -0.040969,  0.136735, -0.16749 ,\n",
       "        0.015138, -0.257993,  0.045962, -0.268149,  0.030862, -0.092176,\n",
       "        0.122345,  0.030217, -0.160942,  0.011762, -0.119691,  0.001137,\n",
       "       -0.01738 ,  0.117686, -0.182717,  0.065644, -0.082618,  0.022352,\n",
       "       -0.164335,  0.145822,  0.041732, -0.074911,  0.218303,  0.069315,\n",
       "        0.057877,  0.024107, -0.066434, -0.064211,  0.110319, -0.115906,\n",
       "       -0.043664,  0.14885 ,  0.02675 ,  0.09146 ,  0.05057 , -0.180723,\n",
       "       -0.064295, -0.083177, -0.008054,  0.10666 ,  0.126736, -0.115527,\n",
       "       -0.01989 ,  0.075149, -0.000993,  0.055185, -0.066153, -0.039885,\n",
       "        0.059212, -0.085952, -0.208585,  0.093467, -0.041614, -0.119066,\n",
       "       -0.040206, -0.005586, -0.143227,  0.07137 , -0.009192, -0.116081,\n",
       "        0.099088, -0.066392,  0.092849,  0.093492, -0.225096, -0.146772,\n",
       "       -0.058909,  0.056219, -0.082881,  0.062284,  0.044458, -0.251119,\n",
       "        0.104715, -0.043641,  0.132329, -0.067922, -0.015381, -0.016145,\n",
       "       -0.114821, -0.030016,  0.033005, -0.013362, -0.086211, -0.127774,\n",
       "       -0.007984,  0.048048, -0.014836, -0.184012, -0.006152, -0.170686,\n",
       "       -0.059291, -0.012643,  0.11829 ,  0.02126 ,  0.015246, -0.138139,\n",
       "        0.267928,  0.018795, -0.138785,  0.308255,  0.044726, -0.102918,\n",
       "       -0.129886,  0.003058, -0.043572,  0.051295, -0.05984 ,  0.002867,\n",
       "       -0.11693 , -0.146097, -0.033237, -0.07334 , -0.024717,  0.056488,\n",
       "        0.033832, -0.248658, -0.191014,  0.022175,  0.059567, -0.146464,\n",
       "       -0.031874, -0.014507, -0.110379,  0.037301,  0.069659,  0.042886,\n",
       "       -0.047623,  0.139237, -0.024608, -0.028311,  0.178302,  0.153792,\n",
       "       -0.000441, -0.291872,  0.049479, -0.181116,  0.049121, -0.102402,\n",
       "        0.257765,  0.055701,  0.068292,  0.000822,  0.083192,  0.016129,\n",
       "        0.206718,  0.098025,  0.22285 , -0.002744, -0.033136, -0.055607,\n",
       "       -0.082307, -0.187616, -0.027721, -0.021265,  0.068379,  0.026259,\n",
       "       -0.068343, -0.034328,  0.089857,  0.034433,  0.10339 ,  0.154465,\n",
       "       -0.003456, -0.053838, -0.185965, -0.019069, -0.243974, -0.074408,\n",
       "        0.149727,  0.041157, -0.071889,  0.106221, -0.055447,  0.154198,\n",
       "       -0.030152, -0.090191,  0.021164, -0.080567,  0.114253,  0.120214,\n",
       "        0.165585,  0.02793 ,  0.020207,  0.030263, -0.046007, -0.04272 ,\n",
       "       -0.099695, -0.310224,  0.112363, -0.090951, -0.041856, -0.008283,\n",
       "       -0.084874, -0.148028,  0.060563, -0.168423, -0.007394,  0.104805,\n",
       "       -0.036561, -0.071372,  0.021624,  0.043165,  0.097281, -0.016033,\n",
       "        0.166581, -0.01694 , -0.014347,  0.138629, -0.000952, -0.008572,\n",
       "       -0.125505,  0.017449, -0.095913, -0.0631  ,  0.050866,  0.004967,\n",
       "       -0.055538, -0.030228,  0.129408, -0.121469, -0.052   ,  0.018903,\n",
       "       -0.097454, -0.022519, -0.010883, -0.051341, -0.014617,  0.016637,\n",
       "        0.087631,  0.063154,  0.001155, -0.162523,  0.031023,  0.005763,\n",
       "       -0.172581,  0.007713,  0.106375,  0.128399,  0.203369, -0.064915,\n",
       "        0.138553,  0.078829, -0.153252,  0.067559,  0.112278, -0.105729,\n",
       "       -0.079211,  0.067912,  0.106474, -0.030243, -0.135996, -0.127359,\n",
       "       -0.041086, -0.039145, -0.107861, -0.092109,  0.117025, -0.067401],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[':)']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
